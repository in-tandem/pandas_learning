{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of USA Auto Sales History in order to predict car prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal: Prepare data and finalize features from the entire data set which can be used to create our modelling algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would be using the data source archived in the location https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data.\n",
    "\n",
    "The following steps would be undertaken as part of the data analysis exercise\n",
    "\n",
    "1. read the data from the remote location\n",
    "2. provide appropriate column headers\n",
    "3. provide description of the data along with division of numeric and non numeric values\n",
    "4. Provide stats such as unique counts, top values, inter quartile range\n",
    "5. Check for non empty values and decide on whether to drop values or replace. Provide justification for each\n",
    "6. Provide data normalization for numeric features. Decide which columns require data normalization\n",
    "7. Provide data bining functionality for categorical numeric or non numeric values. Provide justification for each.\n",
    "8. Perform grouping of features vs price exercises to display relation between price and features\n",
    "9. Provide heat map to display correlation between features. ALso provide any other charts/graphs as deemed necessary.\n",
    "10. Provide coefficient values and p values to decide on which features to finalize for modelling of algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as panda\n",
    "\n",
    "remote_location = 'https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'\n",
    "\n",
    "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
    "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
    "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
    "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of data downloaded is 205 rows * 26 columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## we had a look at the data placed in the remote location and found that the data is without any headers. \n",
    "## hence we pass the arguments, headers = None\n",
    "data = panda.read_csv(remote_location, header = None) \n",
    "\n",
    "data_dimensions = data.shape\n",
    "print(\"The dimensions of data downloaded is %d rows * %d columns\" %(data_dimensions[0], data_dimensions[1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Top ten rows ************\n",
      "   symboling normalized-losses         make fuel-type aspiration num-of-doors  \\\n",
      "0          3                 ?  alfa-romero       gas        std          two   \n",
      "1          3                 ?  alfa-romero       gas        std          two   \n",
      "2          1                 ?  alfa-romero       gas        std          two   \n",
      "3          2               164         audi       gas        std         four   \n",
      "4          2               164         audi       gas        std         four   \n",
      "5          2                 ?         audi       gas        std          two   \n",
      "6          1               158         audi       gas        std         four   \n",
      "7          1                 ?         audi       gas        std         four   \n",
      "8          1               158         audi       gas      turbo         four   \n",
      "9          0                 ?         audi       gas      turbo          two   \n",
      "\n",
      "    body-style drive-wheels engine-location  wheel-base  ...    engine-size  \\\n",
      "0  convertible          rwd           front        88.6  ...            130   \n",
      "1  convertible          rwd           front        88.6  ...            130   \n",
      "2    hatchback          rwd           front        94.5  ...            152   \n",
      "3        sedan          fwd           front        99.8  ...            109   \n",
      "4        sedan          4wd           front        99.4  ...            136   \n",
      "5        sedan          fwd           front        99.8  ...            136   \n",
      "6        sedan          fwd           front       105.8  ...            136   \n",
      "7        wagon          fwd           front       105.8  ...            136   \n",
      "8        sedan          fwd           front       105.8  ...            131   \n",
      "9    hatchback          4wd           front        99.5  ...            131   \n",
      "\n",
      "   fuel-system  bore  stroke compression-ratio horsepower  peak-rpm city-mpg  \\\n",
      "0         mpfi  3.47    2.68               9.0        111      5000       21   \n",
      "1         mpfi  3.47    2.68               9.0        111      5000       21   \n",
      "2         mpfi  2.68    3.47               9.0        154      5000       19   \n",
      "3         mpfi  3.19    3.40              10.0        102      5500       24   \n",
      "4         mpfi  3.19    3.40               8.0        115      5500       18   \n",
      "5         mpfi  3.19    3.40               8.5        110      5500       19   \n",
      "6         mpfi  3.19    3.40               8.5        110      5500       19   \n",
      "7         mpfi  3.19    3.40               8.5        110      5500       19   \n",
      "8         mpfi  3.13    3.40               8.3        140      5500       17   \n",
      "9         mpfi  3.13    3.40               7.0        160      5500       16   \n",
      "\n",
      "  highway-mpg  price  \n",
      "0          27  13495  \n",
      "1          27  16500  \n",
      "2          26  16500  \n",
      "3          30  13950  \n",
      "4          22  17450  \n",
      "5          25  15250  \n",
      "6          25  17710  \n",
      "7          25  18920  \n",
      "8          20  23875  \n",
      "9          22      ?  \n",
      "\n",
      "[10 rows x 26 columns]\n",
      "************ Bottom ten rows ************\n",
      "     symboling normalized-losses   make fuel-type aspiration num-of-doors  \\\n",
      "195         -1                74  volvo       gas        std         four   \n",
      "196         -2               103  volvo       gas        std         four   \n",
      "197         -1                74  volvo       gas        std         four   \n",
      "198         -2               103  volvo       gas      turbo         four   \n",
      "199         -1                74  volvo       gas      turbo         four   \n",
      "200         -1                95  volvo       gas        std         four   \n",
      "201         -1                95  volvo       gas      turbo         four   \n",
      "202         -1                95  volvo       gas        std         four   \n",
      "203         -1                95  volvo    diesel      turbo         four   \n",
      "204         -1                95  volvo       gas      turbo         four   \n",
      "\n",
      "    body-style drive-wheels engine-location  wheel-base  ...    engine-size  \\\n",
      "195      wagon          rwd           front       104.3  ...            141   \n",
      "196      sedan          rwd           front       104.3  ...            141   \n",
      "197      wagon          rwd           front       104.3  ...            141   \n",
      "198      sedan          rwd           front       104.3  ...            130   \n",
      "199      wagon          rwd           front       104.3  ...            130   \n",
      "200      sedan          rwd           front       109.1  ...            141   \n",
      "201      sedan          rwd           front       109.1  ...            141   \n",
      "202      sedan          rwd           front       109.1  ...            173   \n",
      "203      sedan          rwd           front       109.1  ...            145   \n",
      "204      sedan          rwd           front       109.1  ...            141   \n",
      "\n",
      "     fuel-system  bore  stroke compression-ratio horsepower  peak-rpm  \\\n",
      "195         mpfi  3.78    3.15               9.5        114      5400   \n",
      "196         mpfi  3.78    3.15               9.5        114      5400   \n",
      "197         mpfi  3.78    3.15               9.5        114      5400   \n",
      "198         mpfi  3.62    3.15               7.5        162      5100   \n",
      "199         mpfi  3.62    3.15               7.5        162      5100   \n",
      "200         mpfi  3.78    3.15               9.5        114      5400   \n",
      "201         mpfi  3.78    3.15               8.7        160      5300   \n",
      "202         mpfi  3.58    2.87               8.8        134      5500   \n",
      "203          idi  3.01    3.40              23.0        106      4800   \n",
      "204         mpfi  3.78    3.15               9.5        114      5400   \n",
      "\n",
      "    city-mpg highway-mpg  price  \n",
      "195       23          28  13415  \n",
      "196       24          28  15985  \n",
      "197       24          28  16515  \n",
      "198       17          22  18420  \n",
      "199       17          22  18950  \n",
      "200       23          28  16845  \n",
      "201       19          25  19045  \n",
      "202       18          23  21485  \n",
      "203       26          27  22470  \n",
      "204       19          25  22625  \n",
      "\n",
      "[10 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "## we are going to assign headers to the data downloaded.\n",
    "## we are also going to rename the headers all to lowercase for standardization purposes (this is an optional step)\n",
    "## and then we are going to print the first 10 rows and last ten rows for simply eyeballing part of the data\n",
    "\n",
    "data.columns = [ i.lower() for i in headers]\n",
    "\n",
    "print(\"************ Top ten rows ************\")\n",
    "print(data.head(10))\n",
    "\n",
    "\n",
    "\n",
    "print(\"************ Bottom ten rows ************\")\n",
    "print(data.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are done with our steps 1 and 2.\n",
    "\n",
    "A few things to notice so far:\n",
    "\n",
    "1. our data has 205 rows\n",
    "2. our data has 26 columns\n",
    "3. our data has the value that we would like to predict, i.e. price. This point is important since this signifies our learning algorithm can be a supervised learning algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step would be to analyze the data further. We would like to check the data types of the data provided, and check if it requires updating. We would also check the data formats of the data provided and once again, check if it requires updating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205 entries, 0 to 204\n",
      "Data columns (total 26 columns):\n",
      "symboling            205 non-null int64\n",
      "normalized-losses    205 non-null object\n",
      "make                 205 non-null object\n",
      "fuel-type            205 non-null object\n",
      "aspiration           205 non-null object\n",
      "num-of-doors         205 non-null object\n",
      "body-style           205 non-null object\n",
      "drive-wheels         205 non-null object\n",
      "engine-location      205 non-null object\n",
      "wheel-base           205 non-null float64\n",
      "length               205 non-null float64\n",
      "width                205 non-null float64\n",
      "height               205 non-null float64\n",
      "curb-weight          205 non-null int64\n",
      "engine-type          205 non-null object\n",
      "num-of-cylinders     205 non-null object\n",
      "engine-size          205 non-null int64\n",
      "fuel-system          205 non-null object\n",
      "bore                 205 non-null object\n",
      "stroke               205 non-null object\n",
      "compression-ratio    205 non-null float64\n",
      "horsepower           205 non-null object\n",
      "peak-rpm             205 non-null object\n",
      "city-mpg             205 non-null int64\n",
      "highway-mpg          205 non-null int64\n",
      "price                205 non-null object\n",
      "dtypes: float64(5), int64(5), object(16)\n",
      "memory usage: 41.7+ KB\n",
      "object     16\n",
      "float64     5\n",
      "int64       5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## we will check for the data types in the data provided and give a count of the unique data types provided\n",
    "\n",
    "data.info() ## for simple eyeballing of the data provided. gives us overview of datatypes across columns\n",
    "data_types = data.dtypes ##data_types returned is an instance of pandas.DataSeries class\n",
    "\n",
    "## value_counts gives us a result of number of unique data types in the entire data. value_counts is an attribute only present in DataSeries and not in DataFrame\n",
    "\n",
    "print(data_types.value_counts())\n",
    "\n",
    "\n",
    "## a longer way to achieve the above is given below, where we convert it to data frame and reset indices and run a groupby and count command\n",
    "## FYI - data.info() would have also given DataFrame instance automatically.\n",
    "# data_types = panda.DataFrame(data_types, columns = ['column_type'])\n",
    "# data_types.index.name = 'column_name'\n",
    "\n",
    "# data_types.reset_index(inplace = True)\n",
    "# print(data_types.groupby( by = 'column_type', as_index = False).count())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see that the breakdown of data types is as below:\n",
    " \n",
    "object     16\n",
    "\n",
    "float64     5\n",
    "\n",
    "int64       5\n",
    "\n",
    "\n",
    "**********************************************************************************************************\n",
    "\n",
    "Some random observations just based on eyeballing the data and the datatype information above :\n",
    "\n",
    "1. There are too many string or object data types. Statistical analysis proves difficult for non numerical data and we will need to check if data types can be updated\n",
    "\n",
    "2. Data types which require updating (from simple eyeballing the data) are price, normalized losses, horsepower ,etc\n",
    "\n",
    "P.S :Before updating the data types though, we would like to check for empty values and how to handle the same\n",
    "\n",
    "************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "We will now proceed to provide some statistics of numeric data and non numeric data as well\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        symboling  wheel-base      length       width      height  \\\n",
      "count  205.000000  205.000000  205.000000  205.000000  205.000000   \n",
      "mean     0.834146   98.756585  174.049268   65.907805   53.724878   \n",
      "std      1.245307    6.021776   12.337289    2.145204    2.443522   \n",
      "min     -2.000000   86.600000  141.100000   60.300000   47.800000   \n",
      "25%      0.000000   94.500000  166.300000   64.100000   52.000000   \n",
      "50%      1.000000   97.000000  173.200000   65.500000   54.100000   \n",
      "75%      2.000000  102.400000  183.100000   66.900000   55.500000   \n",
      "max      3.000000  120.900000  208.100000   72.300000   59.800000   \n",
      "\n",
      "       curb-weight  engine-size  compression-ratio    city-mpg  highway-mpg  \n",
      "count   205.000000   205.000000         205.000000  205.000000   205.000000  \n",
      "mean   2555.565854   126.907317          10.142537   25.219512    30.751220  \n",
      "std     520.680204    41.642693           3.972040    6.542142     6.886443  \n",
      "min    1488.000000    61.000000           7.000000   13.000000    16.000000  \n",
      "25%    2145.000000    97.000000           8.600000   19.000000    25.000000  \n",
      "50%    2414.000000   120.000000           9.000000   24.000000    30.000000  \n",
      "75%    2935.000000   141.000000           9.400000   30.000000    34.000000  \n",
      "max    4066.000000   326.000000          23.000000   49.000000    54.000000  \n",
      "       normalized-losses    make fuel-type aspiration num-of-doors body-style  \\\n",
      "count                205     205       205        205          205        205   \n",
      "unique                52      22         2          2            3          5   \n",
      "top                    ?  toyota       gas        std         four      sedan   \n",
      "freq                  41      32       185        168          114         96   \n",
      "\n",
      "       drive-wheels engine-location engine-type num-of-cylinders fuel-system  \\\n",
      "count           205             205         205              205         205   \n",
      "unique            3               2           7                7           8   \n",
      "top             fwd           front         ohc             four        mpfi   \n",
      "freq            120             202         148              159          94   \n",
      "\n",
      "        bore stroke horsepower peak-rpm price  \n",
      "count    205    205        205      205   205  \n",
      "unique    39     37         60       24   187  \n",
      "top     3.62   3.40         68     5500     ?  \n",
      "freq      23     20         19       37     4  \n"
     ]
    }
   ],
   "source": [
    "## we will call describe method of pandas data frame to show us data statistics for numeric columns.\n",
    "\n",
    "## the below two lines could have been combined using include =áll keyword, but i prefer separation\n",
    "print(data.describe())\n",
    "\n",
    "print(data.describe(include = 'object'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations we can make from the statistical description of each columns:\n",
    "\n",
    "1. Price, normalized-losses columns have value ?. This needs to be converted to numpy.NaN values so that our data casting can be done\n",
    "\n",
    "\n",
    "2. Curb weight has huge range of values which can lead to curb weight dictating models. Hence we would need to normalize our data\n",
    "\n",
    "\n",
    "3. Make of car Toyota appears 32 times in the 206 rows of data. So we may notice that data of only one model is not collected, our model data is also present. However the maximum amount of data is available for Toyota however this will not lead to skewed results since it appears only 32 times in 206 records (a bit over 10%)\n",
    "\n",
    "\n",
    "4. Engine location topmost value appears 202 times out of 206 records. We may assume engine location at top is a standard feature across cars and hence will not be impacting/dictating price of cars. Hence we may remove this from our data features\n",
    "\n",
    "\n",
    "\n",
    "5. columns such has fuel-type, drive-wheels, aspirations, num-of-cylinders. num-of-doors, engine-type all have very few number of unique values and the top most occuring value appears more than 100 out of total of 205 rows. We would need to carefully check the correlation between these values and price in order to decide if they need to be considered as a data feature. (We would be using pearson coefficient and p value to decide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "At this point we are done with our steps 3 and 4.\n",
    "\n",
    "We would now proceed to deal with missing data\n",
    "\n",
    "1. Show which columns, if any, have missing data\n",
    "\n",
    "2. Price and normalized-losses both have ? as missing values. That needs to be converted to NaN value\n",
    "\n",
    "3. Price is what we need to predict and having a target with blank value does not make sense and hence we would drop that row\n",
    "\n",
    "4. normalized-losses has numeric values. we would replace missing values( at this point numpy.NaN value) with mean of the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normalized-losses': {'missing_count': 41, 'data_type': 'object'}, 'num-of-doors': {'missing_count': 2, 'data_type': 'object'}, 'bore': {'missing_count': 4, 'data_type': 'object'}, 'stroke': {'missing_count': 4, 'data_type': 'object'}, 'horsepower': {'missing_count': 2, 'data_type': 'object'}, 'peak-rpm': {'missing_count': 2, 'data_type': 'object'}, 'price': {'missing_count': 4, 'data_type': 'object'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "data.replace('?', numpy.nan, inplace = True) ## inplace marked as true , hence we are modifying original data\n",
    "\n",
    "temp_null_data_check = data.isnull()\n",
    "empty_column_list = {}\n",
    "\n",
    "for col in temp_null_data_check.columns.tolist():\n",
    "    values = temp_null_data_check[col].values\n",
    "    \n",
    "    if True in values :\n",
    "        empty_column_list[col] = {'missing_count' : list(values).count(True), 'data_type' : data[col].dtype.name} ##dtype is numpy.dtype instance, value is numpy.darray instance\n",
    "        \n",
    "\n",
    "print(empty_column_list)\n",
    "\n",
    "\n",
    "## another way to achieve the above is give below.\n",
    "# empty_values = data.isnull().sum().to_frame()\n",
    "# empty_values= empty_values.assign(column_type = data.dtypes)\n",
    "# empty_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "From the above analysis , we get the following data:\n",
    "\n",
    "-  'normalized-losses': {'missing_count': 41, 'data_type': 'object'}\n",
    "\n",
    "-  'num-of-doors': {'missing_count': 2, 'data_type': 'object'}\n",
    "\n",
    "-  'bore': {'missing_count': 4, 'data_type': 'object'}\n",
    "\n",
    "-  'stroke': {'missing_count': 4, 'data_type': 'object'}\n",
    "\n",
    "-  'horsepower': {'missing_count': 2, 'data_type': 'object'}\n",
    "\n",
    "-  'peak-rpm': {'missing_count': 2, 'data_type': 'object'}\n",
    "\n",
    "-  'price': {'missing_count': 4, 'data_type': 'object'}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the following actions we are going to take based on the above:\n",
    "\n",
    "1. price is our target value for our supervised learning data set. Having empty value for target does not make sense. Similarly, we cant assign average value for the empty cell as that would be incorrect understanding. Hence we are going to drop the row with empty price cell. Since we are dropping this particular row, all our mean calculations will be done only post this step\n",
    "\n",
    "\n",
    "2. we are going to convert the price to float data type\n",
    "\n",
    "\n",
    "\n",
    "3. normalized losses column has a huge number of empty values, close to 20% of our number of rows. From our earlier analysis we also know this is a numeric column. SO we are going to replace empty values with average of the normalized losses and check to see correlation with price later on based on that assumption\n",
    "\n",
    "\n",
    "\n",
    "4. we are also going to convert normalized losses to float64 datatype\n",
    "\n",
    "\n",
    "\n",
    "5. number of doors has a frequency of value 4 for 114 entries and it has only 2 values missing. We are going to replace missing value with the topmost appearing value which is 4. We are also going to convert the column to int64\n",
    "\n",
    "\n",
    "6. Actions for bore, horsepower,stroke,peak-rpm will be similar to step 3 and 4 above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak rpm summation after mean calc 1028635.175879397\n",
      "horsepower summation after mean calc 20782.79396984925\n",
      "stroke summation after mean calc 654.6376142131978\n",
      "bore summation after mean calc 669.4728426395939\n",
      "normalized-losses summation after mean calc 24522.0\n",
      "Successfully completed all empty values replacement\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## we can drop rows where all the values are NaN. THat is empty rows by using the command data.dropna(how='all')\n",
    "\n",
    "## drop row with empty price cell\n",
    " \n",
    "data.dropna(inplace = True, axis =0, subset = ['price'])\n",
    "\n",
    "data['price'] = data['price'].astype('float64')\n",
    "\n",
    "## check to see if the row has been dropped\n",
    "numpy.nan in data['price'].values\n",
    "\n",
    "## convert data types for normalized losses, horsepower, bore, stroke, peak -rpm  since we are going to perform mathematical operation\n",
    "\n",
    "data[['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm']] = data[['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm']].astype('float64')\n",
    "\n",
    "## calculate mean of normalized losses, horsepower, bore, stroke, peak -rpm\n",
    "\n",
    "normalized_losses_mean = data['normalized-losses'].mean()\n",
    "bore_mean              = data['bore'].mean()\n",
    "stroke_mean            = data['stroke'].mean()\n",
    "horsepower_mean        = data['horsepower'].mean()\n",
    "peak_rpm_mean          = data['peak-rpm'].mean()\n",
    "\n",
    "## assign missing values to the individual mean value\n",
    "\n",
    "\n",
    "data['normalized-losses'].replace(to_replace = numpy.nan, value = normalized_losses_mean, inplace =True)\n",
    "data['bore'].replace(to_replace = numpy.nan, value = bore_mean, inplace =True)\n",
    "data['stroke'].replace(to_replace = numpy.nan, value = stroke_mean, inplace =True)\n",
    "data['horsepower'].replace(to_replace = numpy.nan, value = horsepower_mean, inplace =True)\n",
    "data['peak-rpm'].replace(to_replace = numpy.nan, value = peak_rpm_mean, inplace =True)\n",
    "\n",
    "## sklearn.impute.SimpleImputer is in latest version and not available yet.\n",
    "## code in case of SimpleIMputer would have been SimpleImputer(missing = numpy.nan, strategy = 'mean'). and then fit_trasnform\n",
    "\n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# imputer = Imputer(missing_values = numpy.nan, strategy = 'mean', axis = 0)\n",
    "# data = imputer.fit_transform(data.values)\n",
    "\n",
    "print(\"peak rpm summation after mean calc %s\" %data['peak-rpm'].sum())\n",
    "print(\"horsepower summation after mean calc %s\" %data['horsepower'].sum())\n",
    "print(\"stroke summation after mean calc %s\" %data['stroke'].sum())\n",
    "print(\"bore summation after mean calc %s\" %data['bore'].sum())\n",
    "print(\"normalized-losses summation after mean calc %s\" %data['normalized-losses'].sum())\n",
    "## we can use the sklearn.impute.SimpleImputer with strategy mean to transform all the above 4 values at one shot\n",
    "\n",
    "## calculate top value for number of doors and replace empty value with it\n",
    "\n",
    "number_of_doors_top_value = data['num-of-doors'].value_counts().idxmax()\n",
    "\n",
    "data['num-of-doors'].replace(to_replace = numpy.nan, value = number_of_doors_top_value, inplace =True)\n",
    "\n",
    "\n",
    "## run the empty values check again\n",
    "\n",
    "\n",
    "temp_null_data_check = data.isnull()\n",
    "empty_column_list = {}\n",
    "\n",
    "for col in temp_null_data_check.columns.tolist():\n",
    "    values = temp_null_data_check[col].values\n",
    "    \n",
    "    if True in values :\n",
    "        empty_column_list[col] = {'missing_count' : list(values).count(True), 'data_type' : data[col].dtype.name} ##dtype is numpy.dtype instance, value is numpy.darray instance\n",
    "        \n",
    "\n",
    "if not empty_column_list:\n",
    "    print(\"Successfully completed all empty values replacement\")\n",
    "# print(empty_column_list) ## empty dict means our empty values replacement is successfully completed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now that all our data has been cleared of empty values and data types has been assigned correctly, we would start by observing the range of values for numeric inputs and checking to see if certain columns require normalization.\n",
    "\n",
    "In order to do so, we would again describe our data sets for only the numeric values\n",
    "TODO: to be done after question regarding normalization is answered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now proceed to perform certain actions that are deemed necessary (based on business knowledge) eg data binning.\n",
    "\n",
    "We can see horsepower attribute has 57 unique numerical values, however we are not interested in getting exact numeric values. Instead we are interested in calculations based on higher , lower , medium horsepower values. SO we would convert our numeric range of data for horsepower into bins of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###lets draw a histogram for showing data distribution\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sns\n",
    "\n",
    "plot.hist(data['horsepower'], bins = 3)\n",
    "plot.title('Horsepower Distribution')\n",
    "plot.xlabel(\"Horsepower values\")\n",
    "plot.ylabel(\"Count\")\n",
    "\n",
    "plot.show()\n",
    "\n",
    "sns.distplot(data['horsepower'], bins =3, kde=False, rug=True)\n",
    "\n",
    "\n",
    "\n",
    "## get the range of values for horsepower, the min and the max\n",
    "\n",
    "min_horse_power = data['horsepower'].min()\n",
    "max_horse_power = data['horsepower'].max()\n",
    "\n",
    "## set the number of bins you require and provide names for the same\n",
    "\n",
    "bin_labels = ['low', 'medium', 'high']\n",
    "bin_count = 4  ## cut function has default rightmost edge as true.\n",
    "\n",
    "## lets say we knew which numbers to classify as low, medium and high. say from a given range of [0,20]  we \n",
    "## say numbers within 0 - 7 is low, 7 - 14 is medium and 14 - 20 is high. so our bins are essentially [0,7,14,20] i.e one higher\n",
    "## than the number of labels. \n",
    "\n",
    "## we dont want to decide the ranges for bins . we let numpy decide the same\n",
    "## using linspace, which returns evenly spaced numbers using intervals\n",
    "\n",
    "bins = numpy.linspace(start = min_horse_power, stop = max_horse_power, num = 4)\n",
    "# print(bins,min_horse_power,max_horse_power)\n",
    "\n",
    "## convert data into bins\n",
    "\n",
    "data['horsepower-binned'] = panda.cut(data['horsepower'], bins, labels = bin_labels ,include_lowest = True )\n",
    "\n",
    "# print(data[['horsepower','horsepower-binned']].head(20))\n",
    "print(data['horsepower-binned'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "We observe columns such as fuel-type and aspiration have only two unique values and both these values are non-numeric. Since non numeric values prove difficult values for regression analysis we are going to convert these values to a numeric number and change the data type.\n",
    "\n",
    "How do we convert these categorical values to numeric values? Say for fuel type, diesel is assigned a value of 1 and gas is assigned a value of 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## another way of achieving the below is to use the panda.get_dummies method\n",
    "## another technique is to use the map function. v = {'gas' :1, 'diesel' : 0}. data[].map(V)\n",
    "\n",
    "data['fuel-type'] = data['fuel-type'].apply(lambda x: 1 if x == 'gas' else 0 )\n",
    "data['fuel-type'] = data['fuel-type'].astype('int64')\n",
    "\n",
    "\n",
    "data['fuel-type'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['aspiration'] = data['aspiration'].apply(lambda x: 1 if x == 'std' else 0 )\n",
    "data['aspiration'] = data['aspiration'].astype('int64')\n",
    "\n",
    "\n",
    "data['aspiration'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We also observe engine-location also has only two unique values, however since we already decided previously that engine location does not impact the price of the vehicle we are going to ignore updating/changing this particular attribute\n",
    "\n",
    "Time to find out correlation of the remaining attributes and price target.\n",
    "\n",
    "1. we would be drawing regression plots to view the dependence between attribute and target price\n",
    "\n",
    "\n",
    "2. we would calculate the pearson coefficient and p value and mark them as legends on the graph\n",
    "\n",
    "\n",
    "3. We also know that pearson coefficient is sensitive to outliers. Hence we would be plotting boxplots for the attributes to see if there are outliers\n",
    "\n",
    "\n",
    "\n",
    "*** How do we deal with outliers. We may remove rows with outlier values and check how they impact the analysis.\n",
    "\n",
    "\n",
    "\n",
    "Lets decide on the following attributes which would undergo the above analysis:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<li>symboling</li>\n",
    "<li>normalized-losses</li>\n",
    "<li>make</li>\n",
    "<li>fuel-type</li>\n",
    "<li>aspiration</li>\n",
    "<li>num</li>\n",
    "<li>body-style</li>\n",
    "<li>drive-wheels</li>\n",
    "<li>wheel-base</li>\n",
    "<li>length</li>\n",
    "<li>width</li>\n",
    "<li>height</li>\n",
    "<li>curb-weight</li>\n",
    "<li>engine-type</li>\n",
    "<li>num-of-cylinders</li>\n",
    "<li>engine-size</li>\n",
    "<li>fuel-system</li>\n",
    "<li>bore</li>\n",
    "<li>stroke</li>\n",
    "<li>compression-ratio</li>\n",
    "<li>horsepower</li>\n",
    "<li>peak-rpm</li>\n",
    "<li>city-mpg</li>\n",
    "<li>highway-mpg</li>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A couple of the above values are numeric and some are categorical. Hence analysis would be separate for each\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## describe the numeric columns which we think are likely to impact price target \n",
    "## (we would perform the same for categorical values later). we want to confirm our assumption\n",
    "\n",
    "print(data[['symboling', 'normalized-losses' , 'length', 'width', 'height', 'curb-weight', 'engine-size']].describe(include = 'all'))\n",
    "\n",
    "data[['symboling', 'normalized-losses' , 'length', 'width', 'height', 'curb-weight', 'engine-size']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## plotting boxplots for the above 7 attributes to check for outliers and inter quartile ranges\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data[['symboling', 'normalized-losses' , 'length', 'width', 'height', 'curb-weight', 'engine-size']].plot(kind= 'box', figsize =(10,20))\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "We can make the following observations from the box plot above:\n",
    "    \n",
    "    1.  symboling has a very lower range of values with hardly any outliers. Hardly any outliers means our coefficients values are going to be truly representatice of correlation\n",
    "    \n",
    "    \n",
    "    2. width and height are almost in the same range. however length has very different upper scales , eg. min value of length is higher than max value of both these attributes. Its time to normalize these attributes and plot them again\n",
    "    \n",
    "    \n",
    "    3. engine size has quite a few outliers\n",
    "    \n",
    "    \n",
    "Pearson coefficient is meant to be read as below:\n",
    "\n",
    " close to 1 : totally correlated\n",
    " \n",
    " close to 0 : no correlation, values do not impact one another\n",
    " \n",
    " close to -1 : negatively correlated\n",
    " \n",
    " P value is meant to be read as below: ( p value is probability that values are related to one another)\n",
    " \n",
    " < 0.001 : very strong correlation ( 99% probability we can say that values are correlated)\n",
    " \n",
    " < 0.05 : strong correlation ( 95% probability we can say that values are correlated)\n",
    " \n",
    " < 0.1  : there is weak evidence that the correlation is significant, and\n",
    "\n",
    " \\>  0.1 : there is no evidence that the correlation is significant.\n",
    " \n",
    " \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "## checking correlation between symbolizing and target price\n",
    "data['symboling'] = data['symboling'].astype('float64')\n",
    "\n",
    "pearson_coef, p_value = stats.pearsonr(data['symboling'], data['price'])\n",
    "print(\"The Pearson Correlation Coefficient is %s with a P-value of P = %s\" %(pearson_coef, p_value))\n",
    "\n",
    "_data_symboling_to_price = data[['symboling', 'price']]\n",
    "\n",
    "plot.figure(figsize=(15,8))\n",
    "sns.regplot(x = 'symboling' , y ='price' , data = data[['symboling', 'price']])\n",
    "\n",
    "\n",
    "plot.text(x = 2, y = 45000 , s =\"Pearson Correlation Coefficient = %s\"%pearson_coef, fontsize = 12 )\n",
    "plot.text(x = 2, y = 42000 , s =\"P value = %s\"%p_value, fontsize = 12 )\n",
    "\n",
    "## we can see close to - 0.08 correlation coefficient and greater than 0.1 p value number, meaning values are NOT related to each other\n",
    "## what is clearly visible is that symboling is not related to price at all and hence can we ignored as a contributing feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Let us now normalize the values for lengt, width and height and draw the regplto and calculate coefficients\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will use the min max technique for normalization of length, width and height\n",
    "## min-max technique is x-min/max-min. values range betoiwwen 0 and 1\n",
    "## other technique is z score which is x - mean/std deviation. values range betwwen -inf to inf (however typical is 0 to -3 to +3)\n",
    "## the otehr is feature scaling which is x/ max . values range between 0 and 1\n",
    "\n",
    "\n",
    "\n",
    "data['length'] = (data['length'] - data['length'].min())/ (data['length'].max() - data['length'].min())\n",
    "\n",
    "data['width'] = (data['width'] - data['width'].min())/ (data['width'].max() - data['width'].min())\n",
    "\n",
    "data['height'] = (data['height'] - data['height'].min())/ (data['height'].max() - data['height'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drawing plots and calculation of coefficients of each\n",
    "\n",
    "\n",
    "\n",
    "pearson_coef, p_value = stats.pearsonr(data['length'], data['price'])\n",
    "print(\"The Pearson Correlation Coefficient is %s with a P-value of P = %s\" %(pearson_coef, p_value))\n",
    "\n",
    "plot.figure(figsize=(15,15))\n",
    "sns.regplot(x = 'length' , y ='price' , data = data[['length', 'price']])\n",
    "\n",
    "\n",
    "plot.text(x = 1, y = 40000 , s =\"Pearson Correlation Coefficient = %s\"%pearson_coef, fontsize = 12 )\n",
    "plot.text(x = 1, y = 38000 , s =\"P value = %s\"%p_value, fontsize = 12 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## we can see close to 1 correlation coefficient and a very small p value number, meaning values are related to each other\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = stats.pearsonr(data['width'], data['price'])\n",
    "print(\"Width : The Pearson Correlation Coefficient is %s with a P-value of P = %s\" %(pearson_coef, p_value))\n",
    "\n",
    "plot.figure(figsize=(15,15))\n",
    "sns.regplot(x = 'width' , y ='price' , data = data[['width', 'price']])\n",
    "\n",
    "\n",
    "plot.text(x = 1, y = 40000 , s =\"Pearson Correlation Coefficient = %s\"%pearson_coef, fontsize = 12 )\n",
    "plot.text(x = 1, y = 38000 , s =\"P value = %s\"%p_value, fontsize = 12 )\n",
    "\n",
    "\n",
    "## we can see close to 1 correlation coefficient and a very small p value number, meaning values are related to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = stats.pearsonr(data['height'], data['price'])\n",
    "print(\"height : The Pearson Correlation Coefficient is %s with a P-value of P = %s\" %(pearson_coef, p_value))\n",
    "\n",
    "plot.figure(figsize=(15,15))\n",
    "sns.regplot(x = 'height' , y ='price' , data = data[['height', 'price']])\n",
    "\n",
    "\n",
    "plot.text(x = 1, y = 40000 , s =\"Pearson Correlation Coefficient = %s\"%pearson_coef, fontsize = 12 )\n",
    "plot.text(x = 1, y = 38000 , s =\"P value = %s\"%p_value, fontsize = 12 )\n",
    "\n",
    "## we can see close to 0.1 correlation coefficient and greater than 0.01 p value number, meaning values are NOT related to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Based on the above observations we can see that :\n",
    "\n",
    "1. symboling : not related , will not be included in our data features\n",
    "\n",
    "2. height : not related , will not be included in our data features\n",
    "\n",
    "3. length and width :  related , will  be included in our data features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
